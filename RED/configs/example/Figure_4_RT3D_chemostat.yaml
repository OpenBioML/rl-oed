defaults:
  - /environment: chemostat
  - /model: RT3D_agent
  - _self_

wandb_project_name: figure4-example
wandb_team: rl-oed

policy_delay: 2
initial_explore_rate: 1
explore_rate_mul: 1
test_episode: False
save_path: ${hydra:run.dir}
ckpt_freq: 50
load_ckpt_dir_path: null # directory containing agent's checkpoint to load ("agent.pt") + optionally "history.json" from which to resume training

model:
  batch_size: ${eval:'${example.environment.N_control_intervals} * ${example.environment.n_parallel_experiments}'}
  val_learning_rate: 0.0001
  pol_learning_rate: 0.00005
  action_bounds: [0, 1]
  noise_bounds: [-0.25, 0.25]
  noise_std: 0.1
  gamma: 1
  polyak: 0.995
  max_length: 11
  mem_size: 500_000_000
  pol_module_specs:
    - input_size: ${eval:'${example.environment.n_observed_variables} + 1 + ${example.environment.n_controlled_inputs}'}
      layers:
        - layer_type: GRU
          hidden_size: 64
          num_layers: 1
        - layer_type: GRU
          hidden_size: 64
          num_layers: 1
    - input_size: ${eval:'${example.environment.n_observed_variables} + 1 + ${..[0].layers[1].hidden_size}'}
      layers:
        - layer_type: Linear
          output_size: 128
          activation:
            _target_: torch.nn.ReLU
        - layer_type: Linear
          output_size: 128
          activation:
            _target_: torch.nn.ReLU
        - layer_type: Linear
          output_size: ${example.environment.n_controlled_inputs}
          activation:
            _target_: torch.nn.Sigmoid
  val_module_specs:
    - input_size: ${eval:'${example.environment.n_observed_variables} + 1 + ${example.environment.n_controlled_inputs}'}
      layers:
        - layer_type: GRU
          hidden_size: 64
          num_layers: 1
        - layer_type: GRU
          hidden_size: 64
          num_layers: 1
    - input_size: ${eval:'${example.environment.n_observed_variables} + 1 + ${example.environment.n_controlled_inputs} + ${..[0].layers[1].hidden_size}'}
      layers:
        - layer_type: Linear
          output_size: 128
          activation:
            _target_: torch.nn.ReLU
        - layer_type: Linear
          output_size: 128
          activation:
            _target_: torch.nn.ReLU
        - layer_type: Linear
          output_size: 1
          activation:
            _target_: torch.nn.Identity