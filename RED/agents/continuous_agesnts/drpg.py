import torch
from torch import nn
from torch.optim import Adam
from torch.utils.data import DataLoader
from torch.nn import L1Loss
from torch.nn.utils.rnn import pad_sequences
import numpy as np
import copy, os

from .utils import NeuralNetwork

class DRPG_agent():
    def __init__(self, layer_sizes, learning_rate = 0.001, critic=True):
        self.memory = []
        self.layer_sizes = layer_sizes
        self.gamma = 1.

        self.critic = critic
        if critic:
            self.critic_network = self.initialise_network(layer_sizes)
            self.critic_network_opt = Adam(self.opt_Q1_network.parameters(), lr=val_learning_rate)
            self.critic_network_loss = L1Loss()

        self.actor_network = self.initialise_network(layer_sizes)
        self.actor_network_opt = Adam(self.opt_Q1_network.parameters(), lr=val_learning_rate)
        self.actor_network_loss = L1Loss()

        self.values = []
        self.actions = []

        self.states = []
        self.next_states = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.sequences = []
        self.next_sequences = []
        self.all_values = []


    def initialise_network(self, layer_sizes, critic_nw = False):

        '''
        Creates Q network for value function approximation
        '''
        network = NeuralNetwork(layer_sizes, critic_nw)

        return network


    def get_actions(self, inputs):

        states, sequences = inputs


        sequences = torch.pad_sequences(sequences)


        mu, log_std = self.actor_network.predict([np.array(states), sequences])

        print('mu log_std', mu[0], log_std[0])

        actions = mu + torch.mult(torch.normal(mu.size), torch.exp(log_std))
        #print('actions',actions[0])

        return actions

    def loss(self, inputs, actions, returns):
        # Obtain mu and sigma from actor network
        mu, log_std = self.actor_network(inputs)

        # Compute log probability
        log_probability = self.log_probability(actions, mu, log_std)
        print('log probability', log_probability.shape)
        print('returns:', returns.shape)
        # Compute weighted loss
        returns_x_logprob = torch.mult(returns, log_probability)
        loss_actor = self.Q1_network_loss(returns_x_logprob, torch.zeros(returns_x_logprob))
        print('loss actor', loss_actor.shape)
        return loss_actor


    def log_probability(self, actions, mu, log_std):

        EPS = 1e-8
        pre_sum = -0.5 * (((actions - mu) / (torch.exp(log_std) + EPS)) ** 2 + 2 * log_std + np.log(2 * np.pi))

        print('pre sum', pre_sum.shape)
        return torch.sum(pre_sum, axis=1)

    def train(model, train_dataloder, optimizer, criterion, epochs):
        for _ in range(epochs):
            # go through all the batches generated by dataloader
            for i, (inputs, targets) in enumerate(train_dataloder):
                # clear the gradients
                optimizer.zero_grad()
                # compute the model output
                yhat = model(inputs)
                # calculate loss
                loss = criterion(yhat, targets.type(torch.LongTensor))
                # credit assignment
                loss.backward()
                # update model weights
                optimizer.step()

        return model

    def policy_update(self):
        inputs, actions, returns = self.get_inputs_targets()

        print(returns.shape)
        if self.critic:

            expected_returns = self.critic_network(inputs)

            returns -= expected_returns.reshape(-1)
            print(expected_returns.reshape(-1).shape)
            train_dataset = DRPGDataset(inputs, returns)
            train_dataloader = DataLoader(train_dataset)
            self.critic_network = self.train(self.critic_network, train_dataloader, self.critic_network_opt, self.critic_network_loss, epochs=1)        

        loss = self.loss(inputs, actions, returns)
        loss.backward()
        self.critic_network_opt.step()
        loss = self.cri

    def get_inputs_targets(self):
        '''
        gets fitted Q inputs and calculates targets for training the Q-network for episodic training
        '''

        '''
                gets fitted Q inputs and calculates targets for training the Q-network for episodic training
                '''

        # iterate over all exprienc in memory and create fitted Q targets
        for i, trajectory in enumerate(self.memory):

            e_rewards = []
            sequence = [[0]*self.layer_sizes[1]]
            for j, transition in enumerate(trajectory):
                self.sequences.append(copy.deepcopy(sequence))
                state, action, reward, next_state, done, u = transition
                sequence.append(np.concatenate((state, u/1)))
                #one_hot_a = np.array([int(i == action) for i in range(self.layer_sizes[-1])])/10
                self.next_sequences.append(copy.deepcopy(sequence))
                self.states.append(state)
                self.next_states.append(next_state)
                self.actions.append(action)
                self.rewards.append(reward)
                e_rewards.append(reward)
                self.dones.append(done)


            e_values = [e_rewards[-1]]

            for i in range(2, len(e_rewards) + 1):
                e_values.insert(0, e_rewards[-i] + e_values[0] * self.gamma)
            self.all_values.extend(e_values)

        padded = pad_sequences(self.sequences, maxlen = 11, dtype='float64')
        states = np.array(self.states)
        actions = np.array(self.actions)
        all_values = np.array(self.all_values)

        self.sequences = []
        self.states = []
        self.actions = []
        self.all_values = []
        self.memory = []  # reset memory after this information has been extracted

        randomize = np.arange(len(states))
        np.random.shuffle(randomize)

        states = states[randomize]
        actions = actions[randomize]

        padded = padded[randomize]
        all_values = all_values[randomize]

        inputs = [states, padded]
        print('inputs, actions, all_values', inputs[0].shape, inputs[1].shape, actions.shape, all_values.shape)
        return inputs, actions, all_values


    def save_network(self, save_path): # tested
        #print(self.network.layers[1].get_weights())
        torch.save(self.actor_network.state_dict(), 'saved_network.pth')


    def load_network(self, load_path): #tested
        self.policy_network = torch.load(os.path.join(load_path, 'policy_network.pth')) # sometimes this crashes, apparently a bug in keras

